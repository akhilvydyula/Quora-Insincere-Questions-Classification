{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_271.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CxYCwYQXrmJE",
        "zhuZ6qBHrmND",
        "RX87yJFermOP",
        "b6UIHzfermOV",
        "ajkM26UBrmOb",
        "2FVz5nYYrmOf",
        "coDP4PvsrmOo",
        "V-DVQ1KjrmOt",
        "Qy50n6xFrmPO",
        "8_6iw1SirmPX",
        "H05KeqRqrmPi",
        "HFDgtUkQrmP8",
        "lQ_ADcnUrmQD",
        "mvrpXyo8rmQO",
        "-7G_EaRSd-tl"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "600be852c0d28e7c0c5ebb718904ab15a536342c",
        "colab_type": "text",
        "id": "5AvShmqGrmBn"
      },
      "source": [
        "<a id=\"3\"></a> <br>\n",
        "# 1-Problem Definition\n",
        "I think one of the important things when you start a new machine learning project is Defining your problem. that means you should understand business problem.( **Problem Formalization**)\n",
        "> **we will be predicting whether a question asked on Quora is sincere or not.**\n",
        "<a id=\"31\"></a> <br>\n",
        "## 1.1 About Quora\n",
        "Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n",
        "<a id=\"32\"></a> <br>\n",
        "## 1.2 Business View \n",
        "An existential problem for any major website today is how to handle toxic and divisive content. **Quora** wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world.\n",
        "\n",
        "**Quora** is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n",
        "\n",
        "In this kernel, I will develop models that identify and flag insincere questions.we Help Quora uphold their policy of “Be Nice, Be Respectful” and continue to be a place for sharing and growing the world’s knowledge.\n",
        "<a id=\"321\"></a> <br>\n",
        "### 1.2.1 Real world Application Vs Competitions\n",
        "Just a simple comparison between real-world apps with competitions:\n",
        "<img src=\"http://s9.picofile.com/file/8339956300/reallife.png\" height=\"600\" width=\"500\" />\n",
        "<a id=\"33\"></a> <br>\n",
        "## 1.3 What is a insincere question?\n",
        "Is defined as a question intended to make a **statement** rather than look for **helpful answers**.\n",
        "<img src='http://s8.picofile.com/file/8342711526/Quora_moderation.png'>\n",
        "<a id=\"34\"></a> <br>\n",
        "## 1.4 How can we find insincere question?\n",
        "Some characteristics that can signify that a question is insincere:\n",
        "\n",
        "1. **Has a non-neutral tone**\n",
        "    1. Has an exaggerated tone to underscore a point about a group of people\n",
        "    1. Is rhetorical and meant to imply a statement about a group of people\n",
        "1. **Is disparaging or inflammatory**\n",
        "    1. Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n",
        "    1. Makes disparaging attacks/insults against a specific person or group of people\n",
        "    1. Based on an outlandish premise about a group of people\n",
        "    1. Disparages against a characteristic that is not fixable and not measurable\n",
        "1. **Isn't grounded in reality**\n",
        "    1. Based on false information, or contains absurd assumptions\n",
        "    1. Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "556980c672d2f7b2a4ee943b9d13b88de6e41e04",
        "colab_type": "text",
        "id": "M4-tu-8HrmBs"
      },
      "source": [
        "<a id=\"4\"></a> <br>\n",
        "# 2-Problem Feature\n",
        "Problem Definition has three steps that have illustrated in the picture below:\n",
        "\n",
        "1. Aim\n",
        "1. Variable\n",
        "1. Inputs & Outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<a id=\"41\"></a> <br>\n",
        "### 2.1 Aim\n",
        "We will be predicting whether a question asked on Quora is **sincere** or not.\n",
        "\n",
        "\n",
        "<a id=\"42\"></a> <br>\n",
        "### 2.2 Variables\n",
        "\n",
        "1. qid - unique question identifier\n",
        "1. question_text - Quora question text\n",
        "1. target - a question labeled \"insincere\" has a value of 1, otherwise 0\n",
        "\n",
        "<a id=\"43\"></a> <br>\n",
        "### 2.3 Inputs & Outputs\n",
        "we use train.csv and test.csv as Input and we should upload a  submission.csv as Output\n",
        "\n",
        "\n",
        "**<< Note >>**\n",
        "> You must answer the following question:\n",
        "How does your company expect to use and benefit from **your model**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c90e261f3b150e10aaec1f34ab3be768acf7aa25",
        "colab_type": "text",
        "id": "VFTQh6BqrmBw"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab_type": "code",
        "id": "qFixdQUxrmB0",
        "outputId": "930ed083-a94b-4f54-cc60-04ab688d43c9",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as pylab\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import get_dummies\n",
        "\n",
        "import matplotlib as mpl \n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from wordcloud import WordCloud as wc\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "\n",
        "import nltk\n",
        "import json\n",
        "import sys\n",
        "\n",
        "import matplotlib as mpl \n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib \n",
        "\n",
        "import warnings\n",
        "import sklearn as sk\n",
        "\n",
        "import string\n",
        "import scipy\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import string, re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f7fad127cb8ed99cc063b98b3391645263737958",
        "colab_type": "text",
        "id": "sMgP4CcMrmDW"
      },
      "source": [
        "<a id=\"553\"></a> <br>\n",
        "## 4-NLTK stop words\n",
        "Stop words are basically a set of commonly used words in any language, not just English. The reason why stop words are critical to many applications is that, if we remove the words that are very commonly used in a given language, we can focus on the important words instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2NWuEotlXRUr",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aYKCoqSCXkJ2",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "3357ec158943478d584c392bb7702fe7e6d4b355",
        "colab_type": "code",
        "id": "-TCJEN7ZrmDc",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
        "stopWords = set(stopwords.words('english'))\n",
        "words = word_tokenize(data)\n",
        "wordsFiltered = []\n",
        " \n",
        "for w in words:\n",
        "    if w not in stopWords:\n",
        "        wordsFiltered.append(w)\n",
        " \n",
        "print(wordsFiltered)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "581a7ba2ce1ae5dae6c36d54f8999af838c7b80c",
        "colab_type": "text",
        "id": "1G2vVIstrmDm"
      },
      "source": [
        "A module has been imported:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "2cb63648a5f138fe779744f0c52d570f30f84b13",
        "colab_type": "code",
        "id": "O7qvknmwrmDr",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e7528080723ea540729e78b6e135475a870a5618",
        "colab_type": "text",
        "id": "A1mODH1rrmDz"
      },
      "source": [
        "We get a set of English stop words using the line:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "6fbe468728072fb1883e064d0c1e892259fb1c0c",
        "colab_type": "code",
        "id": "QEuex4h9rmD0",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "stopWords = set(stopwords.words('english'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "43843cdaccbe961422631c13d982e13bf25607c6",
        "colab_type": "text",
        "id": "Dx2208LFrmD6"
      },
      "source": [
        "The returned list stopWords contains 153 stop words on my computer.\n",
        "You can view the length or contents of this array with the lines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "53582f8f5ae2871e2cbba4542fc38965b61a5012",
        "colab_type": "code",
        "id": "Qqj_Y5UzrmD7",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "print(len(stopWords))\n",
        "print(stopWords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d25fed9ed1fd0016cea56de8e71b010a0d3176c3",
        "colab_type": "text",
        "id": "6LNbG3cXrmEB"
      },
      "source": [
        "We create a new list called wordsFiltered which contains all words which are not stop words.\n",
        "To create it we iterate over the list of words and only add it if its not in the stopWords list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "3b28823a9862bb263183c6d62a4e91286bfe8d30",
        "colab_type": "code",
        "id": "jYI9nrN4rmED",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "for w in words:\n",
        "    if w not in stopWords:\n",
        "        wordsFiltered.append(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "04ff1a533119d589baee777c21194a951168b0c7",
        "colab_type": "text",
        "id": "wSw9y9DfrmIN"
      },
      "source": [
        "<a id=\"6\"></a> <br>\n",
        "## 5-EDA\n",
        "1. Data Collection\n",
        "1. Visualization\n",
        "1. Data Cleaning\n",
        "1. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "cedecea930b278f86292367cc28d2996a235a169",
        "colab_type": "text",
        "id": "r0sEuIrbrmIQ"
      },
      "source": [
        "<a id=\"61\"></a> <br>\n",
        "## 5.1 Data Collection\n",
        "**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.\n",
        "\n",
        "I start Collection Data by the training and testing datasets into **Pandas DataFrames**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ALcbqe1Zu8AG",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "'''# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')../'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "9269ae851b744856bce56840637030a16a5877e1",
        "colab_type": "code",
        "id": "qXXYMtcMrmIT",
        "outputId": "8fbd24ce-77f5-4418-ca5d-a268241159d6",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#reading train.csv\n",
        "#train = pd.read_csv(\"/content/drive/My Drive/Quora/train.csv\")\n",
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "#reading test.csv\n",
        "#test = pd.read_csv(\"/content/drive/My Drive/Quora/test.csv\")\n",
        "\n",
        "#shape of train dataset\n",
        "print(\"Train shape : \", train.shape)\n",
        "\n",
        "#shape of test dataset\n",
        "print(\"Test shape : \", test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (1306122, 3)\n",
            "Test shape :  (375806, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbvK0aukQlGL",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def clean(text):\n",
        "    \n",
        "    ## Remove puncuation\n",
        "    text = text.translate(string.punctuation)\n",
        "    \n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.lower()\n",
        "    \n",
        "    ## Remove stop words\n",
        "    #text = text.split()\n",
        "    #stops = set(stopwords.words(\"english\"))\n",
        "    #text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "    \n",
        "    #text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = re.sub('[^a-zA-Z]',' ', text)\n",
        "    text = re.sub('  +',' ',text)\n",
        "    \n",
        "    #text = text.split()\n",
        "    #stemmer = SnowballStemmer('english')\n",
        "    #stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    #text = \" \".join(stemmed_words)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "flg7eulBQlGT",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def load_and_prec():\n",
        "    train = pd.read_csv(\"../input/train.csv\")\n",
        "    test = pd.read_csv(\"../input/test.csv\")\n",
        "    print(\"Train shape : \",train.shape)\n",
        "    print(\"Test shape : \",test.shape)\n",
        "    \n",
        "    train['clean_text'] = train['question_text'].apply(clean)\n",
        "    test['clean_text'] = test['question_text'].apply(clean)\n",
        "    \n",
        "    ## split to train and val\n",
        "    train, val = train_test_split(train, test_size=0.08, random_state=2018)\n",
        "\n",
        "\n",
        "    ## fill up the missing values\n",
        "    train_X = train[\"clean_text\"].fillna(\"_##_\").values\n",
        "    val_X = val[\"clean_text\"].fillna(\"_##_\").values\n",
        "    test_X = test[\"clean_text\"].fillna(\"_##_\").values\n",
        "    \n",
        "    ## Tokenize the sentences\n",
        "    tokenizer = Tokenizer(num_words=max_features)\n",
        "    tokenizer.fit_on_texts(list(train_X))\n",
        "    train_X = tokenizer.texts_to_sequences(train_X)\n",
        "    val_X = tokenizer.texts_to_sequences(val_X)\n",
        "    test_X = tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "    ## Pad the sentences \n",
        "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "    val_X = pad_sequences(val_X, maxlen=maxlen)\n",
        "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
        "\n",
        "    ## Get the target values\n",
        "    train_y = train['target'].values\n",
        "    val_y = val['target'].values  \n",
        "    \n",
        "    #shuffling the data\n",
        "    np.random.seed(2018)\n",
        "    trn_idx = np.random.permutation(len(train_X))\n",
        "    val_idx = np.random.permutation(len(val_X))\n",
        "\n",
        "    train_X = train_X[trn_idx]\n",
        "    \n",
        "    val_X = val_X[val_idx]\n",
        "    \n",
        "    train_y = train_y[trn_idx]\n",
        "    \n",
        "    val_y = val_y[val_idx]    \n",
        "    \n",
        "    return train_X, val_X, test_X, train_y, val_y, tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ng_sbJ5MQlGd",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def load_glove(word_index):\n",
        "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    # word_index = tokenizer.word_index\n",
        "    nb_words = min(max_features, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix\n",
        "    \n",
        "def load_fasttext(word_index):    \n",
        "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "    \n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    # word_index = tokenizer.word_index\n",
        "    nb_words = min(max_features, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    \n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        \n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def load_para(word_index):\n",
        "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    \n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    # word_index = tokenizer.word_index\n",
        "    nb_words = min(max_features, len(word_index))\n",
        "    \n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return embedding_matrix\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-wliXqcPQlGt",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def model_lstm_du(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    \n",
        "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    \n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(64, activation=\"relu\")(conc)\n",
        "    \n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vmRjIVBQlG8",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def train_pred(model, epochs=2):\n",
        "    for e in range(epochs):\n",
        "        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))\n",
        "        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
        "\n",
        "        best_thresh = 0.5\n",
        "        best_score = 0.0\n",
        "        for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "            thresh = np.round(thresh, 2)\n",
        "            \n",
        "            score = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
        "            if score > best_score:\n",
        "                best_thresh = thresh\n",
        "                best_score = score\n",
        "\n",
        "        print(\"Val F1 Score: {:.4f}\".format(best_score))\n",
        "\n",
        "    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
        "    return pred_val_y, pred_test_y, best_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4708d70e39d1ae861bbf34411cf03d07f261fceb",
        "colab_type": "code",
        "id": "I3HMvRIlrmIk",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#top of the train dataset rows\n",
        "train.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "f8e7a84ab982504d7263b1812fa66bba78bddbdc",
        "colab_type": "code",
        "id": "avlX5W4FrmIs",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#top of the test dataset rows\n",
        "test.head(5) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "08a94b16129d4c231b64d4691374e18aa80f1d80",
        "colab_type": "code",
        "id": "GD7_SajwrmI6",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#bottom of the train dataset rows\n",
        "train.tail(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ATj590lOw1Pz",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#bottom of the test dataset rows\n",
        "test.tail(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "581b90e6a869c3793472c7edd59091d6d6342fb2",
        "colab_type": "text",
        "id": "CxYCwYQXrmJE"
      },
      "source": [
        "<a id=\"611\"></a> <br>\n",
        "## 5.1.1 Features\n",
        "Features can be from following types:\n",
        "* numeric\n",
        "* categorical\n",
        "* ordinal\n",
        "* datetime\n",
        "* coordinates\n",
        "\n",
        "Find the type of features in **Qoura dataset**?!\n",
        "\n",
        "For getting some information about the dataset you can use **info()** command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "ca840f02925751186f87e402fcb5f637ab1ab8a0",
        "colab_type": "code",
        "id": "YabfhfvMrmJH",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#printing the information about train data\n",
        "print(train.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "4cbcf76344a6e3c8e841ccf1f43bf00d040a06a1",
        "colab_type": "code",
        "id": "iNhizmB4rmJM",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#printing the information about train data\n",
        "print(test.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "73ab30f86273b590a51fc363d9bf78c2709558fa",
        "colab_type": "text",
        "id": "42Ey9MJZrmJQ"
      },
      "source": [
        "<a id=\"612\"></a> <br>\n",
        "## 5.1.2 Explorer Dataset\n",
        "1- Dimensions of the dataset.\n",
        "\n",
        "2- Peek at the data itself.\n",
        "\n",
        "3- Statistical summary of all attributes.\n",
        "\n",
        "4- Breakdown of the data by the class variable.\n",
        "\n",
        "Don’t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "edd043f8feb76cfe51b79785302ca4936ceb7b51",
        "colab_type": "code",
        "id": "OAwhiM_nrmJx",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "type(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pUi0_Swx6VZ",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "type(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "1b8b6f0c962a59e5258e74ed9e740a4aaf7c8113",
        "colab_type": "code",
        "id": "e9wfHVuyrmKI",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train.describe() #description of train dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8280749a19af32869978c61941d1dea306632d71",
        "colab_type": "text",
        "id": "LDpGZFCZrmKk"
      },
      "source": [
        "<a id=\"62\"></a> <br>\n",
        "## 5.2 Data Cleaning\n",
        "When dealing with real-world data, dirty data is the norm rather than the exception. We continuously need to predict correct values, impute missing ones, and find links between various data artefacts such as schemas and records. We need to stop treating data cleaning as a piecemeal exercise (resolving different types of errors in isolation), and instead leverage all signals and resources (such as constraints, available statistics, and dictionaries) to accurately predict corrective actions.\n",
        "\n",
        "The primary goal of data cleaning is to detect and remove errors and **anomalies** to increase the value of data in analytics and decision making. While it has been the focus of many researchers for several years, individual problems have been addressed separately. These include missing value imputation, outliers detection, transformations, integrity constraints violations detection and repair, consistent query answering, deduplication, and many other related problems such as profiling and constraints mining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "675f72fb58d83c527f71819e71ed8e17f81126f5",
        "colab_type": "code",
        "id": "T2eztSbermKp",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a6315bf510cecb907b2d23aad25faf6ccad32ac4",
        "colab_type": "text",
        "id": "RANftG6UrmKm"
      },
      "source": [
        "\n",
        "\n",
        "Good news, it is Zero!\n",
        "\n",
        "As we seen above there is no null values,so we have not to fillna() values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "909d61b33ec06249d0842e6115597bbacf21163f",
        "colab_type": "code",
        "id": "fTF8q4z5rmLT",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train.columns #printing column of train dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "c7937700664991b29bdb0b3f04942c59498da760",
        "colab_type": "code",
        "id": "kpkfh4zWrmLi",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train_target = train['target'].values\n",
        "np.unique(train_target) #number of unique item for Target "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TvKTERrAiYUr"
      },
      "source": [
        "## Text Pre-processing Techniques\n",
        "These techniques may or may not be useful for this competition. Given the fact that is a text competition, i thought that it would be a good oportunity to present them.\n",
        "\n",
        "I have used them before in two papers. A Comparison of Pre-processing Techniques for Twitter Sentiment Analysis and A comparative evaluation of pre-processing techniques and their interactions for twitter sentiment analysis.\n",
        "\n",
        "The full code is on this Github repository with some extra techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VgZDXAQmjthX",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "X_train = train[\"question_text\"].fillna(\"dieter\").values\n",
        "X_test = test[\"question_text\"].fillna(\"dieter\").values\n",
        "y = train[\"target\"]\n",
        "text = train['question_text']\n",
        "\n",
        "for row in text[:10]:\n",
        "    print(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZE6z1X2Ol6-_"
      },
      "source": [
        "# Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R6w2Ynh0lng-",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def removeNumbers(text):\n",
        "    \"\"\" Removes integers \"\"\"\n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    return text\n",
        "\n",
        "text_removeNumbers = pd.DataFrame(columns=['TextBefore', 'TextAfter', 'Changed'])\n",
        "text_removeNumbers['TextBefore'] = text.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IdISHCqSlr2Z",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "for index, row in text_removeNumbers.iterrows():\n",
        "    row['TextAfter'] = removeNumbers(row['TextBefore'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-k2o2i_yl0sM",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "text_removeNumbers['Changed'] = np.where(text_removeNumbers['TextBefore']==text_removeNumbers['TextAfter'], 'no', 'yes')\n",
        "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(text_removeNumbers[text_removeNumbers['Changed']=='yes']), len(text_removeNumbers), 100*len(text_removeNumbers[text_removeNumbers['Changed']=='yes'])/len(text_removeNumbers)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cEyGixkkmATk",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "for index, row in text_removeNumbers[text_removeNumbers['Changed']=='yes'].head().iterrows():\n",
        "    print(row['TextBefore'],'->',row['TextAfter'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vUQuSJm6ngMU",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import re #regular expression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pQLeA6oBmEVT",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def replaceMultiExclamationMark(text):\n",
        "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
        "    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
        "    return text\n",
        "\n",
        "def replaceMultiQuestionMark(text):\n",
        "    \"\"\" Replaces repetitions of question marks \"\"\"\n",
        "    text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
        "    return text\n",
        "\n",
        "def replaceMultiStopMark(text):\n",
        "    \"\"\" Replaces repetitions of stop marks \"\"\"\n",
        "    text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
        "    return text\n",
        "\n",
        "text_replaceRepOfPunct = pd.DataFrame(columns=['TextBefore', 'TextAfter', 'Changed'])\n",
        "text_replaceRepOfPunct['TextBefore'] = text.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "awZUb8tRmJ7G",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "for index, row in text_replaceRepOfPunct.iterrows():\n",
        "    row['TextAfter'] = replaceMultiExclamationMark(row['TextBefore'])\n",
        "    row['TextAfter'] = replaceMultiQuestionMark(row['TextBefore'])\n",
        "    row['TextAfter'] = replaceMultiStopMark(row['TextBefore'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xy4w464smMhS",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "text_replaceRepOfPunct['Changed'] = np.where(text_replaceRepOfPunct['TextBefore']==text_replaceRepOfPunct['TextAfter'], 'no', 'yes')\n",
        "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(text_replaceRepOfPunct[text_replaceRepOfPunct['Changed']=='yes']), len(text_replaceRepOfPunct), 100*len(text_replaceRepOfPunct[text_replaceRepOfPunct['Changed']=='yes'])/len(text_replaceRepOfPunct)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GQzzVwe4nzEI",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "for index, row in text_replaceRepOfPunct[text_replaceRepOfPunct['Changed']=='yes'].head().iterrows():\n",
        "    print(row['TextBefore'],'->',row['TextAfter'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gAJPfTZprpTB",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "text_lowercase = pd.DataFrame(columns=['TextBefore', 'TextAfter', 'Changed'])\n",
        "text_lowercase['TextBefore'] = text.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fo99lTimtjb0",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "for index, row in text_lowercase.iterrows():\n",
        "    row['TextAfter'] = row['TextBefore'].lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iW6fjafBtl3k",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "text_lowercase['Changed'] = np.where(text_lowercase['TextBefore']==text_lowercase['TextAfter'], 'no', 'yes')\n",
        "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(text_lowercase[text_lowercase['Changed']=='yes']), len(text_lowercase), 100*len(text_lowercase[text_lowercase['Changed']=='yes'])/len(text_lowercase)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ASELPEStpOi",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "for index, row in text_lowercase[text_lowercase['Changed']=='yes'].head().iterrows():\n",
        "    print(row['TextBefore'],'->',row['TextAfter'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CmWyUXZcw4NU",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer() #set stemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
        "\n",
        "def tokenize(text):\n",
        "    finalTokens = []\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    for w in tokens:\n",
        "        finalTokens.append(stemmer.stem(w)) # change this to lemmatizer.lemmatize(w) for Lemmatizing\n",
        "    text = \" \".join(finalTokens)\n",
        "    return text\n",
        "\n",
        "text_stemming = pd.DataFrame(columns=['TextBefore', 'TextAfter', 'Changed'])\n",
        "text_stemming['TextBefore'] = text.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qv5CliNaxIV5",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HvYcKRX4w6_m",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "for index, row in text_stemming.iterrows():\n",
        "    row['TextAfter'] = tokenize(row['TextBefore'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w3fta6-Mw-vx",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "text_stemming['Changed'] = np.where(text_stemming['TextBefore'].str.replace(\" \",\"\")==text_stemming['TextAfter'].str.replace(\" \",\"\").str.replace(\"``\",'\"').str.replace(\"''\",'\"'), 'no', 'yes')\n",
        "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(text_stemming[text_stemming['Changed']=='yes']), len(text_stemming), 100*len(text_stemming[text_stemming['Changed']=='yes'])/len(text_stemming)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n9CYdTB6xPGg",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "for index, row in text_stemming[text_stemming['Changed']=='yes'].head().iterrows():\n",
        "    print(row['TextBefore'],'->',row['TextAfter'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "91dda1f631cf4ed362162501aaaac6d19cfd6cc7",
        "colab_type": "text",
        "id": "dz3gpAVkrmMH"
      },
      "source": [
        "<a id=\"63\"></a> <br>\n",
        "## 5.3 Data Preprocessing\n",
        "**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n",
        " \n",
        "Data Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\n",
        "there are plenty of steps for data preprocessing and we just listed some of them in general(Not just for Quora) :\n",
        "1. removing Target column (id)\n",
        "1. Sampling (without replacement)\n",
        "1. Making part of iris unbalanced and balancing (with undersampling and SMOTE)\n",
        "1. Introducing missing values and treating them (replacing by average values)\n",
        "1. Noise filtering\n",
        "1. Data discretization\n",
        "1. Normalization and standardization\n",
        "1. PCA analysis\n",
        "1. Feature selection (filter, embedded, wrapper)\n",
        "1. Etc.\n",
        "\n",
        "What methods of preprocessing can we run on  Quora?! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6c8c838f497c66a227975fb9a2f588e431f0c568",
        "colab_type": "text",
        "id": "I_zxfubQrmMI"
      },
      "source": [
        "\n",
        "### 5.3.1In pandas's data frame you can perform some query such as \"where\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "c8c8d9fd63d9bdb601183aeb4f1435affeb8a596",
        "colab_type": "code",
        "id": "m-LJ_2DxrmMJ",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train.where(train ['target']==1).count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "d517b2b99a455a6b89c238faf1647515b8a67d87",
        "colab_type": "code",
        "id": "oAsLRp_WrmMt",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#Some examples of questions that they are insincere\n",
        "\n",
        "train[train['target']==1].head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4b67d109a0cec1a5475b863bbce8aa3ac9d2d4fb",
        "colab_type": "text",
        "id": "BfM-ZQ7urmMy"
      },
      "source": [
        "<a id=\"631\"></a> <br>\n",
        "## 5.3.2 Is data set imbalance?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "4218d492753322c50142021833efb24cfdfc6ad3",
        "colab_type": "code",
        "id": "BEZx0JM2rmM1",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train_target.mean() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8e058f90ca403f00d91d0405a7d8822dc7d6de55",
        "colab_type": "text",
        "id": "2QNeiUymrmM6"
      },
      "source": [
        "A large part of the data is unbalanced, but **how can we  solve it?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "dc6340ee1b637d192e29cbc8d3744ae6351b9c8b",
        "colab_type": "code",
        "id": "pnU4uC1xrmM8",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train[\"target\"].value_counts()\n",
        "\n",
        "# data is imbalance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "873b2cfdee04b8ba087df1c4bf01ae69ef2f1c52",
        "colab_type": "text",
        "id": "zhuZ6qBHrmND"
      },
      "source": [
        "<a id=\"632\"></a> <br>\n",
        "## 5.3.3 Exploring Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "e445c859d7c43857cfbf370ff20060a5341d3c89",
        "colab_type": "code",
        "id": "NjWGGP5trmNF",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "question = train['question_text']\n",
        "i=0\n",
        "for q in question[:5]:\n",
        "    i=i+1\n",
        "    print('sample '+str(i)+':' ,q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "fa78f61df85a9c76bd092dbf6d6bcec4b6b2631f",
        "colab_type": "code",
        "id": "b-Kd9IitrmNQ",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "text_withnumber = train['question_text']\n",
        "result = ''.join([i for i in text_withnumber if not i.isdigit()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c50c6c2683c5c08a6c9c34b75be61567d5993fa0",
        "colab_type": "text",
        "id": "vhUFRwgCrmNY"
      },
      "source": [
        "<a id=\"632\"></a> <br>\n",
        "## 5.3.4 Some Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c641754f26e07c368596af3054268f1b3b764921",
        "colab_type": "text",
        "id": "8Xf4n_yvrmNb"
      },
      "source": [
        "[NLTK](https://www.nltk.org/) is one of the leading platforms for working with human language data and Python, the module NLTK is used for natural language processing. NLTK is literally an acronym for Natural Language Toolkit.\n",
        "\n",
        "We get a set of **English stop** words using the line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wz7zOF1P0Wk-",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "10ca7d56255b95fc774fff5adf7b4273ec7a1ea2",
        "colab_type": "code",
        "id": "poISxJi2rmNe",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#from nltk.corpus import stopwords\n",
        "eng_stopwords = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3f5af107041b279ce723761f37f4ffebae2b22a3",
        "colab_type": "text",
        "id": "gasAHOi4rmN7"
      },
      "source": [
        "The returned list stopWords contains **179 stop words**  on my computer.\n",
        "You can view the length or contents of this array with the lines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "eca2d53bfae70c55b3b5b0e2c244826465cb478b",
        "colab_type": "code",
        "id": "qNs98oiMrmN9",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "print(len(eng_stopwords))\n",
        "print(eng_stopwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6f049c6d9633200496ed97f8066257849b4824da",
        "colab_type": "text",
        "id": "fVEn1WOArmOC"
      },
      "source": [
        "(http://http://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author) and (https://www.kaggle.com/tunguz/just-some-simple-eda) are:\n",
        "1. Number of words in the text\n",
        "1. Number of unique words in the text\n",
        "1. Number of characters in the text\n",
        "1. Number of stopwords\n",
        "1. Number of punctuations\n",
        "1. Number of upper case words\n",
        "1. Number of title case words\n",
        "1. Average length of the words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f4982fc699bcb147513c247b9f4d86b02902eded",
        "colab_type": "text",
        "id": "51l9ULo8rmOE"
      },
      "source": [
        "## Number of words in the text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nmK1kobD1n3v",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\n",
        "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(10,6), \n",
        "                   title = None, title_size=40, image_color=False):\n",
        "    stopwords = set(STOPWORDS)\n",
        "    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n",
        "    stopwords = stopwords.union(more_stopwords)\n",
        "\n",
        "    wordcloud = WordCloud(background_color='black',\n",
        "                    stopwords = stopwords,\n",
        "                    max_words = max_words,\n",
        "                    max_font_size = max_font_size, \n",
        "                    random_state = 42,\n",
        "                    width=800, \n",
        "                    height=400,\n",
        "                    mask = mask)\n",
        "    wordcloud.generate(str(text))\n",
        "    \n",
        "    plt.figure(figsize=figure_size)\n",
        "    if image_color:\n",
        "        image_colors = ImageColorGenerator(mask);\n",
        "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
        "        plt.title(title, fontdict={'size': title_size,  \n",
        "                                  'verticalalignment': 'bottom'})\n",
        "    else:\n",
        "        plt.imshow(wordcloud);\n",
        "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
        "                                  'verticalalignment': 'bottom'})\n",
        "    plt.axis('off');\n",
        "    plt.tight_layout()  \n",
        "    \n",
        "plot_wordcloud(train[\"question_text\"], title=\"Word Cloud of Questions\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ztcpyAvm8EDq",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "sincere_words = train[train.target==0].question_text.apply(lambda x: x.lower().split())\n",
        "\n",
        "insincere_words = train[train.target==1].question_text.apply(lambda x: x.lower().split())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "afwUQLdu8HxO",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "print('Number of sincere words',len(sincere_words))\n",
        "print('Number of insincere words',len(insincere_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pMmEEprGd-qo"
      },
      "source": [
        "## Number of wordsin the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "5b29fbd86ab48be6bd84fcac6fb6bca84d4b8792",
        "colab_type": "code",
        "id": "-9pi2n_vrmOG",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train[\"num_words\"] = train[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
        "test[\"num_words\"] = test[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "\n",
        "#printing the no of words in train\n",
        "print('maximum of num_words in train',train[\"num_words\"].max())\n",
        "print('min of num_words in train',train[\"num_words\"].min())\n",
        "\n",
        "#printing the no of words in test\n",
        "print(\"maximum of  num_words in test\",test[\"num_words\"].max())\n",
        "print('min of num_words in train',test[\"num_words\"].min())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "83becd8affc2abab2252e065f77c80dc0dcf53be",
        "colab_type": "text",
        "id": "OKPMEGqFrmOL"
      },
      "source": [
        "## Number of unique words in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "72aebb943122982b891c959fa9fa36224adcb2fc",
        "colab_type": "code",
        "id": "JGfGd0h4rmOM",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train[\"num_unique_words\"] = train[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
        "test[\"num_unique_words\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "#printing the no of unique words in train\n",
        "print('maximum of num_unique_words in train',train[\"num_unique_words\"].max())\n",
        "print('mean of num_unique_words in train',train[\"num_unique_words\"].mean())\n",
        "\n",
        "#printing the no of unique words in test\n",
        "print(\"maximum of num_unique_words in test\",test[\"num_unique_words\"].max())\n",
        "print('mean of num_unique_words in train',test[\"num_unique_words\"].mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "cb2719fa417f2c3fabea9b6582081738ecdf678b",
        "colab_type": "text",
        "id": "RX87yJFermOP"
      },
      "source": [
        "## Number of characters in the text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "a7029af9cfed9eb2e624d7177887e111a71054ff",
        "colab_type": "code",
        "id": "xMQmTeXXrmOQ",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "train[\"num_chars\"] = train[\"question_text\"].apply(lambda x: len(str(x)))\n",
        "test[\"num_chars\"] = test[\"question_text\"].apply(lambda x: len(str(x)))\n",
        "\n",
        "#Printing the no of characters in the text\n",
        "print('maximum of num_chars in train',train[\"num_chars\"].max())\n",
        "print(\"maximum of num_chars in test\",test[\"num_chars\"].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ddd289db2420b4f3fee7268fef94926688afd203",
        "colab_type": "text",
        "id": "b6UIHzfermOV"
      },
      "source": [
        "## Number of stopwords in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "086e229b918087420c33b57c7ad51d6723cf70f7",
        "colab_type": "code",
        "id": "aLCXoyMQrmOY",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train[\"num_stopwords\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "test[\"num_stopwords\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "\n",
        "#ptinting the no of stopwords\n",
        "print('maximum of num_stopwords in train',train[\"num_stopwords\"].max())\n",
        "print(\"maximum of num_stopwords in test\",test[\"num_stopwords\"].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "428a009f1a3b00b73ec7d6e8558aebc995e42594",
        "colab_type": "text",
        "id": "ajkM26UBrmOb"
      },
      "source": [
        "## Number of punctuations in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "947abd63c51d74dc33c2891fb1e1b9381d9da23c",
        "colab_type": "code",
        "id": "mjHwQ_ovrmOb",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "train[\"num_punctuations\"] =train['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "test[\"num_punctuations\"] =test['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "\n",
        "print('maximum of num_punctuations in train',train[\"num_punctuations\"].max())\n",
        "print(\"maximum of num_punctuations in test\",test[\"num_punctuations\"].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a93e8fd32c2ff7dffd81f62ff3b6b3a5975d6836",
        "colab_type": "text",
        "id": "2FVz5nYYrmOf"
      },
      "source": [
        "## Number of title case words in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "82c95fcf5848ca383a6a84501fe74fef371392d1",
        "colab_type": "code",
        "id": "MqfdwYXyrmOg",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "train[\"num_words_upper\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "test[\"num_words_upper\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "\n",
        "print('maximum of num_words_upper in train',train[\"num_words_upper\"].max())\n",
        "print(\"maximum of num_words_upper in test\",test[\"num_words_upper\"].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5bc4c2b642cb8adf12fd6dbb01616079a454d384",
        "colab_type": "text",
        "id": "coDP4PvsrmOo"
      },
      "source": [
        "## Number of title case words in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "b938bdcfe7c418f5b4d57c9fd21c77d8bf4d3f06",
        "colab_type": "code",
        "id": "joyrVeNDrmOp",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "train[\"num_words_title\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "test[\"num_words_title\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "\n",
        "print('maximum of num_words_title in train',train[\"num_words_title\"].max())\n",
        "print(\"maximum of num_words_title in test\",test[\"num_words_title\"].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2c81bb79d34d242f74b8ed650a8998efdb29e38b",
        "colab_type": "text",
        "id": "V-DVQ1KjrmOt"
      },
      "source": [
        " ## Average length of the words in the text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "3058236ff8702754ee4132f7eb705dd54f354af4",
        "colab_type": "code",
        "id": "2wZ_pgaArmOu",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "train[\"mean_word_len\"] = train[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test[\"mean_word_len\"] = test[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "print('mean_word_len in train',train[\"mean_word_len\"].max())\n",
        "print(\"mean_word_len in test\",test[\"mean_word_len\"].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c91162602814ba230ab9fe30f9941ac6409133b9",
        "colab_type": "text",
        "id": "MQCmvH1wrmOy"
      },
      "source": [
        "We add some new feature to train and test data set now, print columns agains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "05cae032149a7c79a92a3b2bf80185c483d0e976",
        "colab_type": "code",
        "id": "s-vA8NfFrmOz",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#After adding some features into dataset\n",
        "print(train.columns)\n",
        "\n",
        "train.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "055772bd170aa8018aabd85106b76675802c33b3",
        "colab_type": "text",
        "id": "Qy50n6xFrmPO"
      },
      "source": [
        "<a id=\"64\"></a> <br>\n",
        "## 6-Data Visualization\n",
        "**Data visualization**  is the presentation of data in a pictorial or graphical format. It enables decision makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns.\n",
        "\n",
        "> * Two** important rules** for Data visualization:\n",
        ">     1. Do not put too little information\n",
        ">     1. Do not put too much information\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "1b54931579ed4e3004369a59fe9c6f23b97719de",
        "colab_type": "code",
        "id": "edEQoc94rmPQ",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "ax=sns.countplot(x='target',hue=\"target\", data=train,edgecolor=sns.color_palette(\"dark\", 3))\n",
        "\n",
        "plt.title('Is data set imbalance?');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8b1rbxold-sU"
      },
      "source": [
        "we can see that is is imbalanced dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "be2b936baaa6dcc2d574a861c75584ed04d3589e",
        "colab_type": "text",
        "id": "8_6iw1SirmPX"
      },
      "source": [
        "<a id=\"642\"></a> <br>\n",
        "## 6.1  Pie Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "4a2332f8c87da0a4f8cc31f587ce547470a0d615",
        "colab_type": "code",
        "id": "5AI_lCXurmPY",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "ax=train['target'].value_counts().plot.pie(explode=[0,0.1],shadow=True)\n",
        "ax.set_title('target')\n",
        "ax.set_ylabel('')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PysJVJtDd-sa"
      },
      "source": [
        "The insincere questions are less compare to sincere questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fbe8c50bcc1b632f42dd249e27a9a7c14517fd29",
        "colab_type": "text",
        "id": "H05KeqRqrmPi"
      },
      "source": [
        "<a id=\"643\"></a> <br>\n",
        "## 6.2 Histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "e41be26587175bee1bfe63e43eca1dd3f445c082",
        "colab_type": "code",
        "id": "hCI2Kk9WrmPk",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "f,ax=plt.subplots(1,2,figsize=(10,6))\n",
        "\n",
        "train[train['target']==0].num_words.plot.hist(ax=ax[0],bins=20,edgecolor='white',color='red')\n",
        "\n",
        "ax[0].set_title('target= 0')\n",
        "x1=list(range(0,85,5))\n",
        "\n",
        "ax[0].set_xticks(x1)\n",
        "train[train['target']==1].num_words.plot.hist(ax=ax[1],color='blue',bins=20,edgecolor='white')\n",
        "\n",
        "ax[1].set_title('target= 1')\n",
        "x2=list(range(0,85,5))\n",
        "\n",
        "ax[1].set_xticks(x2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "a7d0a12e7f719781cd09e0d6df6ae1e780a6b1ab",
        "colab_type": "code",
        "id": "QOu1Nup1rmPt",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "f,ax=plt.subplots(1,2,figsize=(10,6))\n",
        "\n",
        "train[['target','num_words']].groupby(['target']).mean().plot.bar(ax=ax[0],color='orange')\n",
        "\n",
        "ax[0].set_title('num_words vs target')\n",
        "sns.countplot('num_words',hue='target',data=train,ax=ax[1],color='black',edgecolor='blue')\n",
        "\n",
        "ax[1].set_title('num_words:target=0 vs target=1')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "7870ad9dc4a007463301513963d6a2c1fe978aa4",
        "colab_type": "code",
        "id": "FRtMPV4mrmP1",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# histograms\n",
        "train.hist(figsize=(20,20),color='skyblue',edgecolor='black')\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a0df20cff46a2bcdeef476e797d1535fd66850c4",
        "colab_type": "text",
        "id": "HFDgtUkQrmP8"
      },
      "source": [
        "<a id=\"644\"></a> <br>\n",
        "## 6.3 Violin Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "fe8ca6c82ce44d745ad1ebc942826cf03e2d9895",
        "colab_type": "code",
        "id": "NZQId71WrmP9",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "sns.violinplot(data=train,x=\"target\", y=\"num_words\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "8436fb0e0a159e9136a44dcd3eb9aae65a4a3b8b",
        "colab_type": "code",
        "id": "wO1u1N-5rmP_",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "sns.violinplot(data=train,x=\"target\", y=\"num_words_upper\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cl3jWZFh6HsW",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "sns.violinplot(data=train,x=\"target\", y=\"num_words_title\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b08771239bf613afa9f89457fdf25c044279940e",
        "colab_type": "text",
        "id": "lQ_ADcnUrmQD"
      },
      "source": [
        "<a id=\"645\"></a> <br>\n",
        "## 6.4 KdePlot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "92f2ce0f1ff05002d196522a6a62579f0dba6ef3",
        "colab_type": "code",
        "id": "5fyWy-lOrmQE",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "sns.FacetGrid(train, hue=\"target\", size=5).map(sns.kdeplot, \"num_words\").add_legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a38e752250257db85554f00b9b440e5d968d4c7a",
        "colab_type": "text",
        "id": "mvrpXyo8rmQO"
      },
      "source": [
        "<a id=\"646\"></a> <br>\n",
        "## 6.5 BoxPlots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "ede2021504e4c4d9eb53c9428f643f11e827f666",
        "colab_type": "code",
        "id": "ZOlN8dMkrmQQ",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train['num_words'].loc[train['num_words']>60] = 60 #truncation for better visuals\n",
        "\n",
        "axes= sns.boxplot(x='target', y='num_words', data=train)\n",
        "\n",
        "axes.set_xlabel('Target', fontsize=12)\n",
        "\n",
        "axes.set_title(\"Number of words in each class\", fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "05a3de95dca3933c118bbaca719a7bd6e4c007cd",
        "colab_type": "code",
        "id": "h_j8LRYGrmQS",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train['num_chars'].loc[train['num_chars']>350] = 350 #truncation for better visuals\n",
        "\n",
        "axes= sns.boxplot(x='target', y='num_chars', data=train)\n",
        "axes.set_xlabel('Target', fontsize=12)\n",
        "axes.set_title(\"Number of num_chars in each class\", fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IReipb-rCzim"
      },
      "source": [
        "We can see that the insincere questions have more number of words as well as characters compared to sincere questions. So this might be a useful feature in our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iuaZ-_VBd-up"
      },
      "source": [
        "# 7 n-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aK0spB3zG_Yg",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# Separating the targets from the feature we will work on\n",
        "X = train.drop(['qid', 'target'], axis=1)\n",
        "y = train['target']\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-7G_EaRSd-tl"
      },
      "source": [
        "#### refer: https://www.kaggle.com/alaric81li215/eda-ml-for-beginners-by-a-beginner-bonus-on-qid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_nRcrp2FS8R",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rFA1IazfGfU6",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "svd = TruncatedSVD(n_components=1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xBx4n-ttGlI7",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "preprocessing_pipe = Pipeline([('vectorizer', vectorizer), ('svd', svd)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f1aIrIsDGyah",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# Building the latent semantic analysis dataframe for sincere and insincere questions\n",
        "\n",
        "lsa_insincere = preprocessing_pipe.fit_transform(X[y==1]['question_text'])\n",
        "topics_insincere = pd.DataFrame(svd.components_)\n",
        "\n",
        "topics_insincere.columns = preprocessing_pipe.named_steps['vectorizer'].get_feature_names()\n",
        "\n",
        "lsa_sincere = preprocessing_pipe.fit_transform(X[y==0]['question_text'])\n",
        "topics_sincere = pd.DataFrame(svd.components_)\n",
        "\n",
        "topics_sincere.columns = preprocessing_pipe.named_steps['vectorizer'].get_feature_names()\n",
        "\n",
        "topics_insincere.shape, topics_sincere.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qg5I8dW8G7Re",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(22,10));\n",
        "\n",
        "topics_sincere.iloc[0].sort_values(ascending=False)[:30].sort_values().plot.barh(ax=axes[0]);\n",
        "topics_insincere.iloc[0].sort_values(ascending=False)[:30].sort_values().plot.barh(ax=axes[1]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0bRZm726Hk8J"
      },
      "source": [
        "Soooo here we are. Some words are obviously more common in insincere questions, like 'white' and 'black', but other words of importance in our LSA are shared by both sincere and insincere questions at the same level, like 'people'. Maybe working on bi-grams or tri-grams will help us define more precislely what an insincere question looks like. But before this, I wanted to try a Truncated SVD with 2 components, to see if I'll be able to link these two components to the results of my bi-grams study later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L3O615raHv9c",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "svd = TruncatedSVD(n_components=2, random_state=42)\n",
        "\n",
        "preprocessing_pipe = Pipeline([('vectorizer', vectorizer), ('svd', svd)])\n",
        "\n",
        "# Building the latent semantic analysis dataframe for sincere and insincere questions\n",
        "\n",
        "lsa_insincere_2 = preprocessing_pipe.fit_transform(X[y==1]['question_text'])\n",
        "topics_insincere_2 = pd.DataFrame(svd.components_)\n",
        "topics_insincere_2.columns = preprocessing_pipe.named_steps['vectorizer'].get_feature_names()\n",
        "\n",
        "lsa_sincere_2 = preprocessing_pipe.fit_transform(X[y==0]['question_text'])\n",
        "topics_sincere_2 = pd.DataFrame(svd.components_)\n",
        "topics_sincere_2.columns = preprocessing_pipe.named_steps['vectorizer'].get_feature_names()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k7mmasp7IIBo",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "vectorizer_13 = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\n",
        "svd_9c = TruncatedSVD(n_components=9, random_state=42)\n",
        "\n",
        "preprocessing_pipe = Pipeline([('vectorizer_13', vectorizer_23), ('svd_9c', svd_9c)])\n",
        "\n",
        "# Building the latent semantic analysis dataframe for insincere questions\n",
        "\n",
        "lsa_insincere_9c = preprocessing_pipe.fit_transform(X[y==1]['question_text'])\n",
        "topics_insincere_9c = pd.DataFrame(svd_9c.components_)\n",
        "topics_insincere_9c.columns = preprocessing_pipe.named_steps['vectorizer_23'].get_feature_names()\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 12))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    topics_insincere_9c.iloc[i].sort_values(ascending=False)[:10].sort_values().plot.barh(ax=ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4-sh_Mceltjd"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Baseline Model:\n",
        "\n",
        "## To start with, let us just build a baseline model (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pHY6yCxeW3bW",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#def tokenize(data):\n",
        "#    tokenized_docs = [word_tokenize(doc.lower()) for doc in data]\n",
        "#    alpha_tokens = [[t for t in doc if t.isalpha() == True] for doc in tokenized_docs]\n",
        "#    stemmer = PorterStemmer ()\n",
        "#    stemmed_tokens = [[stemmer.stem(alpha) for alpha in doc] for doc in alpha_tokens]\n",
        "#    X_stem_as_string = [\" \".join(x_t) for x_t in stemmed_tokens]\n",
        "#    return X_stem_as_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UVtflqVSW4SG",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "X = train['question_text']\n",
        "y = train['target']\n",
        "X_test = test['question_text']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=42, stratify=y)\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lAd7wzrLXbIb",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GC89Y4zgXG6v",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "lr = linear_model.LogisticRegression()\n",
        "\n",
        "tfvec = TfidfVectorizer(stop_words='english', lowercase=False)\n",
        "pipe_lr = Pipeline([\n",
        "    ('vectorizer', tfvec),\n",
        "    ('lr', lr )\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oLppwBcYXsTB",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "pipe_lr.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GLMwKJbKX1cF",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "y_pred_lr = pipe_lr.predict(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZW4PUPJsX4NT",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "cm_lr = metrics.confusion_matrix(y_val, y_pred_lr)\n",
        "\n",
        "ax = plt.gca()\n",
        "sns.heatmap(cm_lr, cmap='Blues', cbar=False, annot=True, xticklabels=y_val.unique(), yticklabels=y_val.unique(), ax=ax);\n",
        "ax.set_xlabel('y_pred');\n",
        "ax.set_ylabel('y_true');\n",
        "ax.set_title('Confusion Matrix');\n",
        "\n",
        "cr = metrics.classification_report(y_val, y_pred_lr)\n",
        "print(cr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_flbTJNfrJl",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "metrics.f1_score(y_pred=y_pred_lr,y_true=y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jqIm2oelbsdQ"
      },
      "source": [
        "# Randomforest model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UjFGPfyUX9y8",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "rf = ensemble.RandomForestClassifier(class_weight='balanced_subsample')\n",
        "tfvec = TfidfVectorizer(stop_words='english', lowercase=False)\n",
        "pipe = Pipeline([\n",
        "    ('vectorizer', tfvec),\n",
        "    ('rf', rf )\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DzsjMMYeYKI_",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BRJPMdPfYMa2",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "y_pred = pipe.predict(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WywqenVsYO6c",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "cm = metrics.confusion_matrix(y_val, y_pred)\n",
        "\n",
        "ax = plt.gca()\n",
        "sns.heatmap(cm, cmap='Blues', cbar=False, annot=True, xticklabels=y_val.unique(), yticklabels=y_val.unique(), ax=ax);\n",
        "ax.set_xlabel('y_pred');\n",
        "ax.set_ylabel('y_true');\n",
        "ax.set_title('Confusion Matrix');\n",
        "\n",
        "cr = metrics.classification_report(y_val, y_pred)\n",
        "print(cr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VuBNRFazYUw_",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "metrics.f1_score(y_pred=y_pred_lr,y_true=y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0mPvOyxLaSOs"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7eTN5I7QlY4",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# some config values \n",
        "embed_size = 300 # how big is each word vector\n",
        "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 70 # max number of words in a question to use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CRFx2yOhQlZO",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def model_lstm_du(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(64, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fUjm18KRQlZR",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def train_pred(model, epochs=2):\n",
        "    for e in range(epochs):\n",
        "        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))\n",
        "        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
        "\n",
        "        best_thresh = 0.5\n",
        "        best_score = 0.0\n",
        "        for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "            \n",
        "            thresh = np.round(thresh, 2)\n",
        "            score = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
        "            if score > best_score:\n",
        "                best_thresh = thresh\n",
        "                \n",
        "                best_score = score\n",
        "\n",
        "        print(\"Val F1 Score: {:.4f}\".format(best_score))\n",
        "\n",
        "    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
        "    return pred_val_y, pred_test_y, best_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uprmSzsAQlZU",
        "outputId": "164b714b-f526-4d5b-d8de-cf051acb0d00",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train_X, val_X, test_X, train_y, val_y, word_index = load_and_prec()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (1306122, 3)\n",
            "Test shape :  (375806, 2)\n",
            "CPU times: user 2min 39s, sys: 0 ns, total: 2min 39s\n",
            "Wall time: 2min 40s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fTEzKdiKQlZa",
        "trusted": true,
        "_kg_hide-output": true,
        "colab": {}
      },
      "source": [
        "%%time\n",
        "embedding_matrix_1 = load_glove(word_index)\n",
        "\n",
        "#embedding_matrix_2 = load_fasttext(word_index)\n",
        "#embedding_matrix_3 = load_para(word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QjyEePGkQlZg",
        "trusted": true,
        "_kg_hide-output": true,
        "colab": {}
      },
      "source": [
        "%%time\n",
        "pred_val_y, pred_test_y, best_score = train_pred(model_lstm_du(embedding_matrix_1), epochs = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U0dszkGAQlZk",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "thresholds = []\n",
        "for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "    \n",
        "    thresh = np.round(thresh, 2)\n",
        "    res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
        "    \n",
        "    thresholds.append([thresh, res])\n",
        "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
        "    \n",
        "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
        "best_thresh = thresholds[0][0]\n",
        "print(\"Best threshold: \", best_thresh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gTSsCBDOQlZo",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "metrics.confusion_matrix(val_y,pred_val_y>best_thresh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CSCJ7VW9aLep"
      },
      "source": [
        "0.683"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hdQpQBrAauCg"
      },
      "source": [
        "# Attention layer and lstm baselinemodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lfkNl3mBQlZs",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "embed_size = 300 # how big is each word vector\n",
        "max_features = 55000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 27 # max number of words in a question to use\n",
        "batch_size = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lNLXha2S6mNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HWR22BRu6mNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#F1 score and CLR\n",
        "\n",
        "# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BD9uns1H6mN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred):\n",
        "    '''\n",
        "    metric from here \n",
        "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "    '''\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EVwlW4cU6mN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM models\n",
        "def model_lstm_atten(embedding_matrix):\n",
        "    \n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    x = Bidirectional(CuDNNLSTM(40, return_sequences=True))(x)\n",
        "    y = Bidirectional(CuDNNGRU(40, return_sequences=True))(x)\n",
        "    \n",
        "    atten_1 = Attention(maxlen)(x) # skip connect\n",
        "    atten_2 = Attention(maxlen)(y)\n",
        "    avg_pool = GlobalAveragePooling1D()(y)\n",
        "    max_pool = GlobalMaxPooling1D()(y)\n",
        "    \n",
        "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n",
        "    conc = Dense(16, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(1, activation=\"sigmoid\")(conc)    \n",
        "\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0ctRIlOLQlZ8",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#Train and predict\n",
        "\n",
        "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
        "def train_pred(model, train_X, train_y, val_X, val_y, epochs=2, callback=None):\n",
        "    for e in range(epochs):\n",
        "        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y), callbacks = callback, verbose=0)\n",
        "        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
        "\n",
        "        best_score = metrics.f1_score(val_y, (pred_val_y > 0.33).astype(int))\n",
        "        print(\"Epoch: \", e, \"-    Val F1 Score: {:.4f}\".format(best_score))\n",
        "\n",
        "    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
        "    print('=' * 60)\n",
        "    return pred_val_y, pred_test_y, best_score\n",
        "\n",
        "\n",
        "train_X, test_X, train_y, word_index = load_and_prec()\n",
        "embedding_matrix_1 = load_glove(word_index)\n",
        "# embedding_matrix_2 = load_fasttext(word_index)\n",
        "embedding_matrix_3 = load_para(word_index)\n",
        "\n",
        "## Simple average: http://aclweb.org/anthology/N18-2031\n",
        "\n",
        "# We have presented an argument for averaging as\n",
        "# a valid meta-embedding technique, and found experimental\n",
        "# performance to be close to, or in some cases \n",
        "# better than that of concatenation, with the\n",
        "# additional benefit of reduced dimensionality  \n",
        "\n",
        "\n",
        "## Unweighted DME in https://arxiv.org/pdf/1804.07983.pdf\n",
        "\n",
        "# “The downside of concatenating embeddings and \n",
        "#  giving that as input to an RNN encoder, however,\n",
        "#  is that the network then quickly becomes inefficient\n",
        "#  as we combine more and more embeddings.”\n",
        "  \n",
        "embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_3], axis = 0)\n",
        "np.shape(embedding_matrix)\n",
        "\n",
        "\n",
        "# https://www.kaggle.com/ryanzhang/tfidf-naivebayes-logreg-baseline\n",
        "\n",
        "def threshold_search(y_true, y_proba):\n",
        "    best_threshold = 0\n",
        "    best_score = 0\n",
        "    for threshold in [i * 0.01 for i in range(100)]:\n",
        "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
        "        if score > best_score:\n",
        "            best_threshold = threshold\n",
        "            best_score = score\n",
        "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
        "    return search_result\n",
        "DATA_SPLIT_SEED = 2018\n",
        "clr = CyclicLR(base_lr=0.001, max_lr=0.002,\n",
        "               step_size=300., mode='exp_range',\n",
        "               gamma=0.99994)\n",
        "\n",
        "train_meta = np.zeros(train_y.shape)\n",
        "test_meta = np.zeros(test_X.shape[0])\n",
        "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_X, train_y))\n",
        "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
        "        X_train = train_X[train_idx]\n",
        "        y_train = train_y[train_idx]\n",
        "        X_val = train_X[valid_idx]\n",
        "        y_val = train_y[valid_idx]\n",
        "        model = model_lstm_atten(embedding_matrix)\n",
        "        pred_val_y, pred_test_y, best_score = train_pred(model, X_train, y_train, X_val, y_val, epochs = 8, callback = [clr,])\n",
        "        train_meta[valid_idx] = pred_val_y.reshape(-1)\n",
        "        test_meta += pred_test_y.reshape(-1) / len(splits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XVcKriucQlaB",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "f1_score(y_true=train_y, y_pred=train_meta > 0.33)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXLpdYQ36mOB",
        "colab_type": "text"
      },
      "source": [
        "# cnn model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UscevWtF6mOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM, concatenate\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, Dropout, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YMi5BUBY6mON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Settings:\n",
        "    \n",
        "# some config values \n",
        "embed_size = 300 # how big is each word vector\n",
        "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 100 # max number of words in a question to use\n",
        "\n",
        "S_DROPOUT = 0.4\n",
        "DROPOUT = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RGgQDpxc6mOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## split to train and val\n",
        "train,  val = train_test_split(train, test_size=0.1, random_state=2018)\n",
        "\n",
        "## fill up the missing values\n",
        "train_X = train[\"question_text\"].fillna(\"_na_\").values\n",
        "val_X = val[\"question_text\"].fillna(\"_na_\").values\n",
        "test_X = test[\"question_text\"].fillna(\"_na_\").values\n",
        "\n",
        "## Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_X))\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "val_X = tokenizer.texts_to_sequences(val_X)\n",
        "test_X = tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "## Pad the sentences \n",
        "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
        "\n",
        "## Get the target values\n",
        "train_y = train['target'].values\n",
        "val_y = val['target'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6jZ9Wc6c6mOX",
        "colab_type": "code",
        "outputId": "5e3db10c-0c87-4629-fe81-c670c32ab3d2",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n",
        "\n",
        "del embeddings_index; gc.collect() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "awZcOZVU6mOe",
        "colab_type": "code",
        "outputId": "85f8cce4-fdcd-4637-9bff-7fc3ce16d996",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\n",
        "        \n",
        "del embeddings_index; gc.collect()    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Dzi1TTuA6mOj",
        "colab_type": "code",
        "outputId": "506a3f94-5531-4048-d97d-2c07e118acd3",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\n",
        "\n",
        "del embeddings_index; gc.collect()   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9G2zG40R6mOt",
        "colab_type": "code",
        "outputId": "bc341043-0afa-4240-cd2d-ee09cd9a7932",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "EMBEDDING_FILE = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
        "embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix_4 = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    if word in embeddings_index:\n",
        "        embedding_vector = embeddings_index.get_vector(word)\n",
        "        embedding_matrix_4[i] = embedding_vector\n",
        "        \n",
        "del embeddings_index; gc.collect()  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ryrntafp6mOw",
        "colab_type": "code",
        "outputId": "d76df2d6-06cc-4f66-b6fe-52ea16cb6e3e",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4), axis=1)  \n",
        "del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\n",
        "gc.collect()\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cP04athc6mO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/yekenot/2dcnn-textclassifier\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "\n",
        "filter_sizes = [1,2,3,5]\n",
        "num_filters = 36\n",
        "\n",
        "inp = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, embed_size * 4, weights=[embedding_matrix])(inp)\n",
        "x = SpatialDropout1D(S_DROPOUT)(x)\n",
        "x = Reshape((maxlen, embed_size * 4, 1))(x)\n",
        "\n",
        "maxpool_pool = []\n",
        "for i in range(len(filter_sizes)):\n",
        "    conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size * 4),\n",
        "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
        "    maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
        "\n",
        "z = Concatenate(axis=1)(maxpool_pool)   \n",
        "z = Flatten()(z)\n",
        "z = Dropout(DROPOUT)(z)\n",
        "\n",
        "outp = Dense(1, activation=\"sigmoid\")(z)\n",
        "\n",
        "model = Model(inputs=inp, outputs=outp)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hmsjuWPD6mO4",
        "colab_type": "code",
        "outputId": "fe6e8f23-4842-462b-fd3a-e7eff034563e",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1057958 samples, validate on 117551 samples\n",
            "Epoch 1/2\n",
            "1057958/1057958 [==============================] - 304s 288us/step - loss: 0.1194 - acc: 0.9536 - val_loss: 0.1055 - val_acc: 0.9577\n",
            "Epoch 2/2\n",
            "1057958/1057958 [==============================] - 303s 286us/step - loss: 0.1005 - acc: 0.9601 - val_loss: 0.1041 - val_acc: 0.9593\n",
            "CPU times: user 1min 39s, sys: 4min 8s, total: 5min 48s\n",
            "Wall time: 10min 8s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5aa0b1ecf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F8z0V1Gt6mO9",
        "colab_type": "code",
        "outputId": "9e265f7b-5dbd-43ff-9e07-184aa3c919f3",
        "colab": {}
      },
      "source": [
        "pred_val_cnn_y = model.predict([val_X], batch_size=1024, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117551/117551 [==============================] - 15s 125us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zzBQ0rxN6mPA",
        "colab_type": "code",
        "outputId": "35c7eb78-304b-4bb9-88ad-b433711e2651",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "pred_test_cnn_y = model.predict([test_X], batch_size=1024, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375806/375806 [==============================] - 46s 123us/step\n",
            "CPU times: user 3.74 s, sys: 3.38 s, total: 7.12 s\n",
            "Wall time: 46.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tYQfwcli6mPD",
        "colab_type": "code",
        "outputId": "452c186f-4e5b-4ab7-f86a-52c9a99715a0",
        "colab": {}
      },
      "source": [
        "pred_val_y = pred_val_cnn_y  # two random numbers :)\n",
        "pred_test_y = pred_test_cnn_y \n",
        "\n",
        "thresholds = []\n",
        "for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "    thresh = np.round(thresh, 2)\n",
        "    res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
        "    thresholds.append([thresh, res])\n",
        "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
        "    \n",
        "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
        "best_thresh = thresholds[0][0]\n",
        "print(\"Best threshold: \", best_thresh)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score at threshold 0.1 is 0.5656863169749209\n",
            "F1 score at threshold 0.11 is 0.5764946983872405\n",
            "F1 score at threshold 0.12 is 0.5875887336844515\n",
            "F1 score at threshold 0.13 is 0.5969505043396668\n",
            "F1 score at threshold 0.14 is 0.6068626979161653\n",
            "F1 score at threshold 0.15 is 0.61504468231366\n",
            "F1 score at threshold 0.16 is 0.6220618453365409\n",
            "F1 score at threshold 0.17 is 0.6271324956583921\n",
            "F1 score at threshold 0.18 is 0.6321211175277041\n",
            "F1 score at threshold 0.19 is 0.6367334002429877\n",
            "F1 score at threshold 0.2 is 0.6409059196050019\n",
            "F1 score at threshold 0.21 is 0.6448206412280224\n",
            "F1 score at threshold 0.22 is 0.64960760473085\n",
            "F1 score at threshold 0.23 is 0.6523710878450255\n",
            "F1 score at threshold 0.24 is 0.6548983993642865\n",
            "F1 score at threshold 0.25 is 0.6568125611088745\n",
            "F1 score at threshold 0.26 is 0.6587481804949054\n",
            "F1 score at threshold 0.27 is 0.6602250633358865\n",
            "F1 score at threshold 0.28 is 0.6627684964200476\n",
            "F1 score at threshold 0.29 is 0.6629620681323991\n",
            "F1 score at threshold 0.3 is 0.6636152813377273\n",
            "F1 score at threshold 0.31 is 0.6656808379544054\n",
            "F1 score at threshold 0.32 is 0.6674976628233095\n",
            "F1 score at threshold 0.33 is 0.6684710769811796\n",
            "F1 score at threshold 0.34 is 0.6686183415006682\n",
            "F1 score at threshold 0.35 is 0.6683375104427737\n",
            "F1 score at threshold 0.36 is 0.6677477944992215\n",
            "F1 score at threshold 0.37 is 0.6681505728314239\n",
            "F1 score at threshold 0.38 is 0.6679902058103369\n",
            "F1 score at threshold 0.39 is 0.6673344017094017\n",
            "F1 score at threshold 0.4 is 0.6663069139966273\n",
            "F1 score at threshold 0.41 is 0.6642574662484658\n",
            "F1 score at threshold 0.42 is 0.6621742419033211\n",
            "F1 score at threshold 0.43 is 0.6608852504509504\n",
            "F1 score at threshold 0.44 is 0.6589635854341738\n",
            "F1 score at threshold 0.45 is 0.6569270943609288\n",
            "F1 score at threshold 0.46 is 0.6555160142348754\n",
            "F1 score at threshold 0.47 is 0.653503733486502\n",
            "F1 score at threshold 0.48 is 0.651503078594712\n",
            "F1 score at threshold 0.49 is 0.6494762288477035\n",
            "F1 score at threshold 0.5 is 0.646381093057607\n",
            "Best threshold:  0.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rVhp5WWembbq"
      },
      "source": [
        "# Pretty Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S296CkStmaVS",
        "outputId": "a89e3e8d-0158-49bc-d847-3a4697be07bd",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "    \n",
        "x = PrettyTable()\n",
        "\n",
        "x.field_names = [\"Logistic regression\", \"Random forest model\", \"lstm\", \"Attention layer and lstm \",\"cnn\"]\n",
        "\n",
        "x.add_row([0.506,0.5066,0.683,0.672,0.6686])\n",
        "\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------+---------------------+-------+---------------------------+--------+\n",
            "| Logistic regression | Random forest model |  lstm | Attention layer and lstm  |  cnn   |\n",
            "+---------------------+---------------------+-------+---------------------------+--------+\n",
            "|        0.506        |        0.5066       | 0.683 |           0.672           | 0.6686 |\n",
            "+---------------------+---------------------+-------+---------------------------+--------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}